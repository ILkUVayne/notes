# go并发调度策略

> go 1.21

## 1. 主动调度

主动调度就是当前的g放弃CPU执行权，将其放入到全局运行队列中，因为它还是可以继续运行的，只是我们主动放弃了，等待下次被调度程序执行。

Gosched函数

在用户代码中通过调用runtime.Gosched()函数发生主动调度. Gosched函数会调用mcall从当前g的栈切换到m的g0栈执行gosched_m函数

/usr/local/go_src/21/go/src/runtime/proc.go:336

~~~go
func Gosched() {
    // checkTimeouts 为空实现
    checkTimeouts()
    // 切换用户栈到g0栈，并执行gosched_m函数
    mcall(gosched_m)
}
~~~

gosched_m函数

gosched_m调用goschedImpl挂起用户协程g

/usr/local/go_src/21/go/src/runtime/proc.go:3764

~~~go
func gosched_m(gp *g) {
    // 如果跟踪已启用,开启跟踪调度
    if traceEnabled() {
        traceGoSched()
    }
    // gp为准备挂起的当前协程，即调用runtime.Gosched()函数所在的协程g
    goschedImpl(gp)
}
~~~

goschedImpl函数

1. 修改挂起协程的状态为_Grunnable
2. 解除gp和当前m的绑定关系
3. 将gp放入全局可执行队列中去
4. 开启一轮新的调度

/usr/local/go_src/21/go/src/runtime/proc.go:3748

~~~go
func goschedImpl(gp *g) {
    // 获取准备挂起g(gp)的状态，它的状态为目前处于_Grunning，因为它正在运行
    status := readgstatus(gp)
    // 检查gp的状态是否合法，如果不为_Grunning状态说明状态出现了异常
    if status&^_Gscan != _Grunning {
        dumpgstatus(gp)
        throw("bad g status")
    }
    // 将gp（用户协程）的状态从_Grunning改变为_Grunnable
    casgstatus(gp, _Grunning, _Grunnable)
    // 解除gp和当前m的绑定关系
    dropg()
    lock(&sched.lock)
    // 将gp放入全局可执行队列中去
    globrunqput(gp)
    unlock(&sched.lock)
    // 开启一轮新的调度
    schedule()
}
~~~

## 2. 被动调度

被动调度就是一个g被迫让出CPU执行权，有哪些情况会让出CPU使用权，我们常见的有channel发生阻塞、网络IO阻塞、执行垃圾回收而暂停用户程序执行等。为了提供CPU的使用率，与其发生阻塞导致CPU干等不如让出CPU给其他可以执行的程序使用。

1. 其他协程调用gopark函数，发生被动调度，并维护协程状态
2. 当达成唤醒条件时，调用goready函数，使被阻塞的g重新恢复为可运行状态_Grunnable，并放入p的本地可执行队列中去等待执行
3. 会调用wakep函数尝试唤醒（新建）一个m执行任务

gopark函数

发生被动调度时（如channel发生阻塞），会调用gopark函数，使g被动调度,该函数会调用mcall函数切换到g0栈，并调用park_m函数

/usr/local/go_src/21/go/src/runtime/proc.go:381

~~~go
func gopark(unlockf func(*g, unsafe.Pointer) bool, lock unsafe.Pointer, reason waitReason, traceReason traceBlockReason, traceskip int) {
    if reason != waitReasonSleep {
        checkTimeouts() // timeouts may expire while two goroutines keep the scheduler busy
    }
    mp := acquirem()
    gp := mp.curg
    status := readgstatus(gp)
    if status != _Grunning && status != _Gscanrunning {
        throw("gopark: bad g status")
    }
    mp.waitlock = lock
    mp.waitunlockf = unlockf
    gp.waitreason = reason
    mp.waitTraceBlockReason = traceReason
    mp.waitTraceSkip = traceskip
    releasem(mp)
    // can't do anything that might move the G between Ms here.
    // 切换到g0栈上执行park_m函数
    mcall(park_m)
}
~~~

park_m函数

1. 修改gp（当前被动调度的用户协程）状态从_Grunning到_Gwaiting
2. 解绑用户协程和m的绑定关系
3. 如果unlockf函数存在，尝试执行，若返回false，则恢复用户协程状态并执行
4. unlockf函数不存在或者返回true，直接开启一轮新的调度

/usr/local/go_src/21/go/src/runtime/proc.go:3721

~~~go
func park_m(gp *g) {
    // g0.m
    mp := getg().m
    
    if traceEnabled() {
        traceGoPark(mp.waitTraceBlockReason, mp.waitTraceSkip)
    }
    
    // N.B. Not using casGToWaiting here because the waitreason is
    // set by park_m's caller.
    // 修改gp（当前被动调度的用户协程）状态从_Grunning到_Gwaiting
    casgstatus(gp, _Grunning, _Gwaiting)
    // 解绑用户协程和m的绑定关系
    dropg()
    // mp.waitunlockf函数是发生被动调度时，调用gopark函数时传入的unlockf
    // 如果mp.waitunlockf函数返回false，则恢复用户协程执行任务，否则开启一轮新的调度
    if fn := mp.waitunlockf; fn != nil {
        ok := fn(gp, mp.waitlock)
        mp.waitunlockf = nil
        mp.waitlock = nil
        if !ok {
            if traceEnabled() {
                traceGoUnpark(gp, 2)
            }
            // 将状态重新修改为_Grunnable，并执行
            casgstatus(gp, _Gwaiting, _Grunnable)
            execute(gp, true) // Schedule it back, never returns.
        }
    }
    // 开启一轮新的调度
    schedule()
}
~~~

goready函数

goready完成的功能是将一个g状态修改为_Grunnable，并将它加入待运行队列，等待被调度程序运行

/usr/local/go_src/21/go/src/runtime/proc.go:407

~~~go
func goready(gp *g, traceskip int) {
    // 切换到g0栈，并执行ready函数
    systemstack(func() {
        ready(gp, traceskip, true)
    })
}
~~~

ready函数

1. 修改gp状态为_Grunnable（可运行状态）
2. 将gp放入本地可运行队列中等待执行
3. 调用wakep函数尝试唤醒（创建）一个新的m执行任务

/usr/local/go_src/21/go/src/runtime/proc.go:890

~~~go
func ready(gp *g, traceskip int, next bool) {
    if traceEnabled() {
        traceGoUnpark(gp, traceskip)
    }
    // 获取gp状态
    status := readgstatus(gp)
    
    // Mark runnable.
    mp := acquirem() // disable preemption because it can be holding p in a local var
    // 检查gp的状态是不是_Gwaiting，如果不是说明gp的状态出现了异常
    if status&^_Gscan != _Gwaiting {
        dumpgstatus(gp)
        throw("bad g->status in ready")
    }
    
    // status is Gwaiting or Gscanwaiting, make Grunnable and put on runq
    // 设置gp状态为_Grunnable（可运行状态）
    casgstatus(gp, _Gwaiting, _Grunnable)
    // 将gp放入本地可运行队列中
    runqput(mp.p.ptr(), gp, next)
	// 尝试唤醒一个新的m执行任务
    wakep()
    releasem(mp)
}
~~~

## 3. 抢占调度

sysmon函数

为了确保每个g都有机会被调度执行，保证调度的公平性，在初始化的时候会启动一个特殊的线程来执行监控任务（sysmon函数），sysmon在runtime.main函数被启动(proc.go:169)。下面看sysmon具体处理逻辑。sysmon函数是一个死循环函数，在第一轮循环的时候休眠20微妙，之后每轮循环中休眠时间加倍，直到最大休眠时间达到10毫秒。在循环中会检查是否有准备就绪的网络，并将其放入到全局队列中，也会进行抢占处理,按时间强制执行gc等操作。这里主要关注抢占处理retake函数，具体怎么抢占都是由该函数实现的。

/usr/local/go_src/21/go/src/runtime/proc.go:5515

~~~go
func sysmon() {
    lock(&sched.lock)
    sched.nmsys++
    checkdead()
    unlock(&sched.lock)
    
    lasttrace := int64(0)
    idle := 0 // how many cycles in succession we had not wokeup somebody
    delay := uint32(0)
    
    for {
        if idle == 0 { // start with 20us sleep...
            delay = 20
        } else if idle > 50 { // start doubling the sleep after 1ms...
            delay *= 2
        }
        if delay > 10*1000 { // up to 10ms
            delay = 10 * 1000
        }
        usleep(delay)
        
        now := nanotime()
        // gc等操作
        if debug.schedtrace <= 0 && (sched.gcwaiting.Load() || sched.npidle.Load() == gomaxprocs) {
            lock(&sched.lock)
            if sched.gcwaiting.Load() || sched.npidle.Load() == gomaxprocs {
                syscallWake := false
                next := timeSleepUntil()
                if next > now {
                    sched.sysmonwait.Store(true)
                    unlock(&sched.lock)
                    // Make wake-up period small enough
                    // for the sampling to be correct.
                    sleep := forcegcperiod / 2
                    if next-now < sleep {
                        sleep = next - now
                    }
                    shouldRelax := sleep >= osRelaxMinNS
                    if shouldRelax {
                        osRelax(true)
                    }
                    syscallWake = notetsleep(&sched.sysmonnote, sleep)
                    if shouldRelax {
                        osRelax(false)
                    }
                    lock(&sched.lock)
                    sched.sysmonwait.Store(false)
                    noteclear(&sched.sysmonnote)
                }
                if syscallWake {
                    idle = 0
                    delay = 20
                }
            }
            unlock(&sched.lock)
        }
        
        lock(&sched.sysmonlock)
        // Update now in case we blocked on sysmonnote or spent a long time
        // blocked on schedlock or sysmonlock above.
        now = nanotime()
        
        // trigger libc interceptors if needed
        if *cgo_yield != nil {
            asmcgocall(*cgo_yield, nil)
        }
        // poll network if not polled for more than 10ms
        lastpoll := sched.lastpoll.Load()
        // 如果超过10ms没有进行netpoll,强制进行一次netpoll，如果获取到了可运行的g,插入g的全局列表
        if netpollinited() && lastpoll != 0 && lastpoll+10*1000*1000 < now {
            sched.lastpoll.CompareAndSwap(lastpoll, now)
            list := netpoll(0) // non-blocking - returns list of goroutines
            if !list.empty() {
                incidlelocked(-1)
                // 把找到的可运行的g列表插入到全局队列
                injectglist(&list)
                incidlelocked(1)
            }
        }
        if GOOS == "netbsd" && needSysmonWorkaround {
            if next := timeSleepUntil(); next < now {
                startm(nil, false, false)
            }
        }
        if scavenger.sysmonWake.Load() != 0 {
            // Kick the scavenger awake if someone requested it.
            scavenger.wake()
        }
        // retake P's blocked in syscalls
        // and preempt long running G's
        // 重新获取在系统调用中阻塞的p并抢占长时间运行的g
        if retake(now) != 0 {
            idle = 0
        } else {
            idle++
        }
        // check if we need to force a GC
        // 检查是否需要启动垃圾回收程序
        if t := (gcTrigger{kind: gcTriggerTime, now: now}); t.test() && forcegc.idle.Load() {
            lock(&forcegc.lock)
            forcegc.idle.Store(false)
            var list gList
            list.push(forcegc.g)
            injectglist(&list)
            unlock(&forcegc.lock)
        }
        if debug.schedtrace > 0 && lasttrace+int64(debug.schedtrace)*1000000 <= now {
            lasttrace = now
            schedtrace(debug.scheddetail > 0)
        }
        unlock(&sched.sysmonlock)
    }
}
~~~

retake函数

p在_Prunning状态，说明与p关联的m上的g正在被运行，判断g运行时间是否超过10毫秒，如果超过了调用preemptone进行抢占。这个抢占是一个异步处理，preemptone设置当前被抢占的g的preempt字段为true,并将stackguard0字段设置为0xfffffade,并没有将当前g暂停执行的逻辑。Go在函数调用的时候会设置安全点，这个是编译器为我们插入的代码。在函数调用的时候会检查stackguard0的大小，而决定是否调用runtime.morestack_noctxt函数。morestack_noctxt是汇编编写的，实现如下，它直接调用morestack函数，morestack会将当前调度信息保存起来，然后切换到g0栈执行newstack函数。newstack函数主要完成两项工作：一是检查是否响应sysmon发起的抢占请求，二是检查栈是否需要进行扩容。newstack检查被抢占g的状态，如果处于抢占状态，调用gopreempt_m将被抢占的gp切换出去放入到全局g运行队列。gopreempt_m是goschedImpl的简单包装，真正处理逻辑在goschedImpl函数，逻辑同主动调度

p在_Psyscall状态，说明与p关联的m上的g正在进行系统调用。只要下面的三个条件有任何一个不满足，就会对处于_Psyscall状态的p进行抢占。

1. p的运行队列中还有等待运行的g，需要及时的让p的本地队列中的g得到调度，而当前的g又处在系统调用中，无法执行其他的g,因此将当前的p抢占过来运行其他的g.
2. 当前没有空闲的p(sched.npidle为0),也没有自旋的m(sched.nmspinning为0)，说明大家都在忙，这时抢占当前正处于系统调用中而实际上用不上的p,分配给其他线程用于调度执行其他的g.
3. 从上一次监控线程留意到p对应的m处于系统调用中到现在已经超过了10毫秒，说明系统调用已花费了比较长的时间，需要进行抢占。使得retake函数的返回值不等于0，让sysmon线程保持20微妙的轮询检查周期，提高监控的实时性。

当上述三个条件有任何一个不满足时，会将p的状态从_Psyscall修改为_Pidle，然后调用handoffp函数，将与进入系统调用的m绑定的p分离。handoff会对当前的条件进行检查，如果满足下面的条件则会调用startm函数启动新的工作线程来与当前的p进行关联，执行可运行的g.具体条件有以下几种：

1. _p_的本地队列或全局队列中有运行的g,立即启动工作线程对g进行调度
2. 当前处于在垃圾回收标记阶段，启动线程运行gc work
3. 当前没有进行自旋的m也没有空闲的p,启动一个线程m进行自旋
4. 这是最后一个运行的p并且没有人在轮询网络，则需要唤醒另一个m来轮询网络
最后，如果上述三个条件都满足，说明当前比较空闲，将p放入到P的全局空闲队列中即可。

/usr/local/go_src/21/go/src/runtime/proc.go:5672

~~~go
func retake(now int64) uint32 {
    n := 0
    lock(&allpLock)
    // 遍历所有的p,因为m运行g需要绑定一个p,检查每个与p绑定的m中运行的g是否需要被抢占
    for i := 0; i < len(allp); i++ {
        pp := allp[i]
        if pp == nil {
            // 如果procresize已经增长了allp但还没有创建新的Ps，就会发生这种情况。
            continue
        }
        // &pp.sysmontick用于监控线程(sysmon)记录被监控的p的运行时间和系统调用时间
        // 以及运行g时的计数器值和进行系统调用时计数器值
        pd := &pp.sysmontick
        // 获取p的状态保存到s中
        s := pp.status
        sysretake := false
        // 如果当前的p正在运行或者处于系统调用中，需要检查是否需要进行抢占
        if s == _Prunning || s == _Psyscall {
            // Preempt G if it's running for too long.
            // 每进行一次调度，pp.schedtick会+1，也就是每切换一个g运行，schedtick会+1
            t := int64(pp.schedtick)
            // 如果pd.schedtick和t不等，也就是pd.schedtick和_p_.schedtick不等
            // 说明p上有进行过调度操作，即切换过g运行，重新更新pd的值schedtick和schedwhen
            // 为下一次判断做准备
            if int64(pd.schedtick) != t {
                pd.schedtick = uint32(t)
                pd.schedwhen = now
            } else if pd.schedwhen+forcePreemptNS <= now {
                // 走到这里说明pd.schedtick与t相等，说明从pd.schedwhen到now这段时间
                // 没有发生过调度，也就是在这段时间，同一个g一直在运行，检查这个g运行的时间
                // 是否超过了10毫秒，就是schedwhen+forcePreemptNS比当前时间小，说明已经超过了

                // 对与_p_绑定的m中运行的g进行抢占
                preemptone(pp)
                // In case of syscall, preemptone() doesn't
                // work, because there is no M wired to P.
                sysretake = true
            }
        }
        // p处在系统调用中，需要检查是否进行抢占
        if s == _Psyscall {
            // Retake P from syscall if it's there for more than 1 sysmon tick (at least 20us).
            // pp.syscalltick 用于记录系统调用的次数，在完成系统调用之后加 1
            t := int64(pp.syscalltick)
            // 如果sysretake为false并且p的pd中记录的系统调用tick和当前p的系统调用tick不等
            // 说明进行过调度，不用进行抢占，直接更新pd的值
            if !sysretake && int64(pd.syscalltick) != t {
                pd.syscalltick = uint32(t)
                pd.syscallwhen = now
                continue
            }
            // 同时满足下面3个条件，并进行抢占：
            // 1. pp的本地运行队列和runnext都没有g
            // 2. 当前进行自旋的m数量+当前空闲的p的数量之和大于0
            // 3. 进入系统调用的时间到现在还不够10毫秒
            if runqempty(pp) && sched.nmspinning.Load()+sched.npidle.Load() > 0 && pd.syscallwhen+10*1000*1000 > now {
                continue
            }
            // Drop allpLock so we can take sched.lock.
            unlock(&allpLock)
            incidlelocked(-1)
            // 将pp的状态从_Psyscall修改为_Pidle
            if atomic.Cas(&pp.status, s, _Pidle) {
                if traceEnabled() {
                    traceGoSysBlock(pp)
                    traceProcStop(pp)
                }
                // n记录处于在系统调用中的p并且需要被抢占的总数
                n++
                // 系统调用tick+1
				pp.syscalltick++
                // 将与进入系统调用的m绑定的p分离
                handoffp(pp)
            }
            incidlelocked(1)
            lock(&allpLock)
        }
    }
    unlock(&allpLock)
    return uint32(n)
}
~~~

